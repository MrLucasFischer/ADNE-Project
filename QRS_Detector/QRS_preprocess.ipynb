{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QRS training data Preprocessor\n",
    "\n",
    "Data preprocessor to build the training and validation data for our neural network\n",
    "\n",
    "This notebook implementes all the necessary steps to run the computations needed to create the training and test data files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wfdb\n",
    "import numpy as np\n",
    "import pickle as pkl\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the parabola function\n",
    "\n",
    "This function will create a parabola around a spike in order to give it more width so it can be more easily detected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# auxiliary function\n",
    "def parabola(a,n,r):\n",
    "    \"\"\"\n",
    "    Creates a parabola around the position of a spike specified in 'a'\n",
    "    Params:\n",
    "        a - A vector specifying peak positions\n",
    "        n - The length of the target vector to generate\n",
    "        r - The radius of the parabola\n",
    "    \"\"\"\n",
    "    assert n>2*r\n",
    "    y = np.zeros(n, dtype = np.float32)\n",
    "    x= np.array(range(2,2*r+1))\n",
    "    for i in a:\n",
    "        if i > r-1 and i <= n-r:\n",
    "            y[i-r+1:i+r] = ((r+1)**2-(x-r-1)**2)/(r+1)**2\n",
    "        elif i < r:\n",
    "            y[:i+r] = ((r+1)**2-(x[r-i-1:]-r-1)**2)/(r+1)**2\n",
    "        elif i<n:\n",
    "            y[i-r+1:] = ((r+1)**2-(x[:r-1+(n-i)]-r-1)**2)/(r+1)**2\n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing the files\n",
    "\n",
    "We iterate all the files and for each of them we read channels II and V1.\n",
    "\n",
    "After reading the channels we separate them into two distinct arrays in order to then join them into one 1D array.\n",
    "\n",
    "We filter out undesired lines, i.e., lines that do not have a QRS symbol specified in `qrs_symbs` list.\n",
    "\n",
    "The negative positions in the `qrs_symbs` list are also filtered out.\n",
    "\n",
    "Once the lines are filtered we create a parabola around the spikes of our labels to better identify them, this completes the preprocessing of the data and we are now ready to create an output dictionary to be serialized into a file with `pickle`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing of the training datafiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./data/Training/I01\n",
      "./data/Training/I02\n",
      "./data/Training/I03\n",
      "./data/Training/I04\n",
      "./data/Training/I05\n",
      "./data/Training/I06\n",
      "./data/Training/I07\n",
      "./data/Training/I08\n",
      "./data/Training/I09\n",
      "./data/Training/I10\n",
      "./data/Training/I11\n",
      "./data/Training/I12\n",
      "./data/Training/I13\n",
      "./data/Training/I14\n",
      "./data/Training/I15\n",
      "./data/Training/I16\n",
      "./data/Training/I17\n",
      "./data/Training/I18\n",
      "./data/Training/I19\n",
      "./data/Training/I20\n",
      "./data/Training/I21\n",
      "./data/Training/I22\n",
      "./data/Training/I23\n",
      "./data/Training/I24\n",
      "./data/Training/I25\n",
      "./data/Training/I26\n",
      "./data/Training/I27\n",
      "./data/Training/I28\n",
      "./data/Training/I29\n",
      "./data/Training/I30\n",
      "./data/Training/I31\n",
      "./data/Training/I32\n",
      "./data/Training/I33\n",
      "./data/Training/I34\n",
      "./data/Training/I35\n",
      "./data/Training/I36\n",
      "./data/Training/I37\n",
      "./data/Training/I38\n",
      "./data/Training/I39\n",
      "./data/Training/I40\n",
      "./data/Training/I41\n",
      "./data/Training/I42\n",
      "./data/Training/I43\n",
      "./data/Training/I44\n",
      "./data/Training/I45\n",
      "./data/Training/I46\n",
      "./data/Training/I47\n",
      "./data/Training/I48\n",
      "./data/Training/I49\n",
      "./data/Training/I50\n",
      "./data/Training/I51\n",
      "./data/Training/I52\n",
      "./data/Training/I53\n",
      "./data/Training/I54\n",
      "./data/Training/I55\n",
      "./data/Training/I56\n",
      "./data/Training/I57\n",
      "./data/Training/I58\n",
      "./data/Training/I59\n",
      "./data/Training/I60\n",
      "./data/Training/I61\n",
      "./data/Training/I62\n",
      "./data/Training/I63\n",
      "./data/Training/I64\n",
      "./data/Training/I65\n",
      "./data/Training/I66\n",
      "./data/Training/I67\n",
      "./data/Training/I68\n",
      "./data/Training/I69\n",
      "./data/Training/I70\n",
      "./data/Training/I71\n",
      "./data/Training/I72\n",
      "./data/Training/I73\n",
      "./data/Training/I74\n",
      "./data/Training/I75\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, 76):\n",
    "    \n",
    "    file_path = f\"./data/Training/I{i:02}\"\n",
    "    print(file_path)\n",
    "    output_file_name = f\"./processed_data/Training/I{i:02}\"\n",
    "    try:\n",
    "        # Reading the channels of interest\n",
    "        signal, info = wfdb.rdsamp(file_path, channel_names = [\"II\", \"V1\"])\n",
    "\n",
    "        # Separating the two signals so we can put them in one dimension\n",
    "        signal_II = signal[:, 0]\n",
    "        signal_V1 = signal[:, 1]\n",
    "\n",
    "\n",
    "        # Reading the annotations\n",
    "        annotations = wfdb.rdann(file_path, \"atr\")\n",
    "        symbol_positions = annotations.sample\n",
    "        symbol_list = annotations.symbol\n",
    "\n",
    "        # Filtering out all the lines that do not have a QRS symbol\n",
    "\n",
    "        qrs_symbs = ['N','L','R','B','A','a','J','S','V','r','F','e','j', 'n', 'E', '/', 'f', 'Q',' ?']\n",
    "\n",
    "        qrs_symbol_positions = [symbol_positions[idx] for idx, symb in enumerate(symbol_list) if symb in qrs_symbs]\n",
    "\n",
    "        # Some peak positions are negative values, we are filtering these negative values out\n",
    "        qrs_symbol_positions = [item for item in qrs_symbol_positions if item >= 0]\n",
    "        \n",
    "        target_vec = parabola(qrs_symbol_positions, len(signal), 3)\n",
    "\n",
    "        output_dict = {\n",
    "            \"features\": signal_II + signal_V1,\n",
    "            \"label\": target_vec\n",
    "        }\n",
    "\n",
    "        pkl.dump(output_dict, open(output_file_name, \"wb\"), protocol=pkl.HIGHEST_PROTOCOL)\n",
    "    except:\n",
    "        print(f\"Error on file {file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example on how to read a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.94444444, -1.97385621, -1.95751634, ...,  4.44117647,\n",
       "        4.41176471,  4.45751634])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dict = pkl.load(open(\"./processed_data/Training/I01\", \"rb\"))\n",
    "data_dict[\"features\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing of the test datafiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./data/Test/101\n",
      "./data/Test/102\n",
      "./data/Test/103\n",
      "./data/Test/104\n",
      "./data/Test/105\n",
      "./data/Test/106\n",
      "./data/Test/107\n",
      "./data/Test/108\n",
      "./data/Test/109\n",
      "./data/Test/110\n",
      "Error on file ./data/Test/110\n",
      "./data/Test/111\n",
      "./data/Test/112\n",
      "./data/Test/113\n",
      "./data/Test/114\n",
      "./data/Test/115\n",
      "./data/Test/116\n",
      "./data/Test/117\n",
      "./data/Test/118\n",
      "./data/Test/119\n",
      "./data/Test/120\n",
      "Error on file ./data/Test/120\n",
      "./data/Test/121\n",
      "./data/Test/122\n",
      "./data/Test/123\n",
      "./data/Test/124\n",
      "./data/Test/125\n",
      "Error on file ./data/Test/125\n",
      "./data/Test/126\n",
      "Error on file ./data/Test/126\n",
      "./data/Test/127\n",
      "Error on file ./data/Test/127\n",
      "./data/Test/128\n",
      "Error on file ./data/Test/128\n",
      "./data/Test/129\n",
      "Error on file ./data/Test/129\n",
      "./data/Test/130\n",
      "Error on file ./data/Test/130\n",
      "./data/Test/131\n",
      "Error on file ./data/Test/131\n",
      "./data/Test/132\n",
      "Error on file ./data/Test/132\n",
      "./data/Test/133\n",
      "Error on file ./data/Test/133\n",
      "./data/Test/134\n",
      "Error on file ./data/Test/134\n",
      "./data/Test/135\n",
      "Error on file ./data/Test/135\n",
      "./data/Test/136\n",
      "Error on file ./data/Test/136\n",
      "./data/Test/137\n",
      "Error on file ./data/Test/137\n",
      "./data/Test/138\n",
      "Error on file ./data/Test/138\n",
      "./data/Test/139\n",
      "Error on file ./data/Test/139\n",
      "./data/Test/140\n",
      "Error on file ./data/Test/140\n",
      "./data/Test/141\n",
      "Error on file ./data/Test/141\n",
      "./data/Test/142\n",
      "Error on file ./data/Test/142\n",
      "./data/Test/143\n",
      "Error on file ./data/Test/143\n",
      "./data/Test/144\n",
      "Error on file ./data/Test/144\n",
      "./data/Test/145\n",
      "Error on file ./data/Test/145\n",
      "./data/Test/146\n",
      "Error on file ./data/Test/146\n",
      "./data/Test/147\n",
      "Error on file ./data/Test/147\n",
      "./data/Test/148\n",
      "Error on file ./data/Test/148\n",
      "./data/Test/149\n",
      "Error on file ./data/Test/149\n",
      "./data/Test/150\n",
      "Error on file ./data/Test/150\n",
      "./data/Test/151\n",
      "Error on file ./data/Test/151\n",
      "./data/Test/152\n",
      "Error on file ./data/Test/152\n",
      "./data/Test/153\n",
      "Error on file ./data/Test/153\n",
      "./data/Test/154\n",
      "Error on file ./data/Test/154\n",
      "./data/Test/155\n",
      "Error on file ./data/Test/155\n",
      "./data/Test/156\n",
      "Error on file ./data/Test/156\n",
      "./data/Test/157\n",
      "Error on file ./data/Test/157\n",
      "./data/Test/158\n",
      "Error on file ./data/Test/158\n",
      "./data/Test/159\n",
      "Error on file ./data/Test/159\n",
      "./data/Test/160\n",
      "Error on file ./data/Test/160\n",
      "./data/Test/161\n",
      "Error on file ./data/Test/161\n",
      "./data/Test/162\n",
      "Error on file ./data/Test/162\n",
      "./data/Test/163\n",
      "Error on file ./data/Test/163\n",
      "./data/Test/164\n",
      "Error on file ./data/Test/164\n",
      "./data/Test/165\n",
      "Error on file ./data/Test/165\n",
      "./data/Test/166\n",
      "Error on file ./data/Test/166\n",
      "./data/Test/167\n",
      "Error on file ./data/Test/167\n",
      "./data/Test/168\n",
      "Error on file ./data/Test/168\n",
      "./data/Test/169\n",
      "Error on file ./data/Test/169\n",
      "./data/Test/170\n",
      "Error on file ./data/Test/170\n",
      "./data/Test/171\n",
      "Error on file ./data/Test/171\n",
      "./data/Test/172\n",
      "Error on file ./data/Test/172\n",
      "./data/Test/173\n",
      "Error on file ./data/Test/173\n",
      "./data/Test/174\n",
      "Error on file ./data/Test/174\n",
      "./data/Test/175\n",
      "Error on file ./data/Test/175\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, 76):\n",
    "    \n",
    "    file_path = f\"./data/Test/1{i:02}\"\n",
    "    print(file_path)\n",
    "    output_file_name = f\"./processed_data/Test/I{i:02}\"\n",
    "    try:\n",
    "        # Reading the channels of interest\n",
    "        signal, info = wfdb.rdsamp(file_path)\n",
    "\n",
    "        # Separating the two signals so we can put them in one dimension\n",
    "        signal_II = signal[:, 0]\n",
    "        signal_V1 = signal[:, 1]\n",
    "\n",
    "\n",
    "        # Reading the annotations\n",
    "        annotations = wfdb.rdann(file_path, \"atr\")\n",
    "        symbol_positions = annotations.sample\n",
    "        symbol_list = annotations.symbol\n",
    "\n",
    "        # Filtering out all the lines that do not have a QRS symbol\n",
    "\n",
    "        qrs_symbs = ['N','L','R','B','A','a','J','S','V','r','F','e','j', 'n', 'E', '/', 'f', 'Q',' ?']\n",
    "\n",
    "        qrs_symbol_positions = [symbol_positions[idx] for idx, symb in enumerate(symbol_list) if symb in qrs_symbs]\n",
    "\n",
    "        # Some peak positions are negative values, we are filtering these negative values out\n",
    "        qrs_symbol_positions = [item for item in qrs_symbol_positions if item >= 0]\n",
    "        \n",
    "        target_vec = parabola(qrs_symbol_positions, len(signal), 3)\n",
    "\n",
    "        output_dict = {\n",
    "            \"features\": signal_II + signal_V1,\n",
    "            \"label\": target_vec\n",
    "        }\n",
    "\n",
    "        pkl.dump(output_dict, open(output_file_name, \"wb\"), protocol=pkl.HIGHEST_PROTOCOL)\n",
    "    except:\n",
    "        print(f\"Error on file {file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
