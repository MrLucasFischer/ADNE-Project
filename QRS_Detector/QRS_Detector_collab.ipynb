{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"QRS_Detector_collab.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"N7GJ9TLwzVSj","colab_type":"text"},"source":["# QRS Detector\n","\n","This notebook implements the actual QRS Detector. In this notebook there are two networks present, a fully conected feed forward network and a recurrent network, with the aim of comparing the two models and verifying which is more indicated for the type of data we have"]},{"cell_type":"code","metadata":{"id":"ZhmQIcyWzhNI","colab_type":"code","colab":{}},"source":["# importing required libraries\n","import numpy as np\n","import tensorflow as tf\n","from tensorflow.python.keras import layers\n","import matplotlib.pyplot as plt\n","import pickle as pkl\n","import random\n","import math"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"jId_h31027mB","colab_type":"code","colab":{}},"source":["from google.colab import drive\n","#drive.mount('/content/drive/')\n","from glob import glob"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"h1CJ9K5821yC","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":63},"outputId":"eeb34ca6-5f23-4a8e-c70e-97e13a44f084","executionInfo":{"status":"ok","timestamp":1559846615308,"user_tz":-60,"elapsed":1866,"user":{"displayName":"Joana Martins","photoUrl":"","userId":"01072178703692360990"}}},"source":["!ls drive/'My Drive'/'Colab Notebooks'/processed_data\n","main_path = 'drive/My Drive/Colab Notebooks/processed_data/'\n","weights_path = 'drive/My Drive/Colab Notebooks/network_weights/'"],"execution_count":16,"outputs":[{"output_type":"stream","text":["Test  Training\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"UVBz9RZmzmB4","colab_type":"text"},"source":["## Sampling Function\n","\n","The objective is to retrieve a random cropping of both the signal and the target from all files specified for the training/validation set. This helps us have a more well-versed dataset for training and validating our models."]},{"cell_type":"code","metadata":{"id":"LSfAr_ABzq7s","colab_type":"code","colab":{}},"source":["# auxiliary function\n","# extracts from a long np-array (2-rows) a (2-rows)-random segment with a fixed length (seqL*ninputs)\n","def selectFrom1ecg(ecgBdata, seqL, ninputs, feed_forward = True, training = True, testing = False):\n","    \"\"\"\n","    x: An array with vairous files, channels and examples\n","    seqL:  number of timesteps to be used in recurrent nn\n","    ninput : is number of inputs in each timestep\n","    file_indexes: A list of the file indexes for training or validation set\n","    \"\"\"\n","    segmentL  = seqL * ninputs\n","    numChan = 3\n","    \n","    if(training):\n","        random_file_idx = random.randint(0, 57)\n","    else:\n","        random_file_idx = random.randint(0, 9)\n","    \n","    if(testing):\n","        random_file_idx = random.randint(0, 48)\n","        \n","    inpOutSegment = tf.random_crop(ecgBdata[random_file_idx],[numChan, segmentL])\n","    \n","    if(feed_forward):\n","        channelII = inpOutSegment[0,:]\n","        channelV1 = inpOutSegment[1,:]\n","        target = inpOutSegment[2,:]\n","        inputs = tf.concat((channelII, channelV1), axis = -1)\n","        return inputs,target\n","    else:\n","        transposed = tf.transpose(inpOutSegment)\n","        \n","        inputs = transposed[:, :-1]\n","        target = transposed[:, -1]\n","        inputs = tf.reshape(inputs, (seqL, -1))\n","        \n","        # We need to re-transpose the target to turn int back into a one-row vector\n","        target = tf.transpose(target)\n","        \n","        return inputs, target"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PxL8mgKIzuB7","colab_type":"text"},"source":["## Dataset array creation\n","\n","In this section we create the main dataset array containing all the training files. Each file has two input signals (channelII and channelV1) and a target signal."]},{"cell_type":"code","metadata":{"id":"tDltqghDzu9y","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1173},"outputId":"7a7f0d40-8e1d-49b9-9e9e-1c1e13245d32","executionInfo":{"status":"ok","timestamp":1559846313249,"user_tz":-60,"elapsed":6046,"user":{"displayName":"Joana Martins","photoUrl":"","userId":"01072178703692360990"}}},"source":["dataset_array = []\n","\n","files_not_to_read = [4,17,35,44,57,72,74]\n","index_counter = 0\n","for i in range(1, 76):\n","    \n","    if i not in files_not_to_read:\n","        file_path = f\"Training/I{i:02}\"\n","        file_path = main_path + file_path\n","        print(file_path)\n","        file_data = pkl.load(open(file_path, \"rb\"))        \n","        index_counter = index_counter + 1\n","        \n","        info = [file_data[\"channelII\"], file_data[\"channelV1\"], file_data[\"label\"]]\n","        info = np.array(info)\n","        info = info.astype(np.float32)\n","        dataset_array.append(info)\n","\n","ecgs_array = np.array(dataset_array)"],"execution_count":8,"outputs":[{"output_type":"stream","text":["drive/My Drive/Colab Notebooks/processed_data/Training/I01\n","drive/My Drive/Colab Notebooks/processed_data/Training/I02\n","drive/My Drive/Colab Notebooks/processed_data/Training/I03\n","drive/My Drive/Colab Notebooks/processed_data/Training/I05\n","drive/My Drive/Colab Notebooks/processed_data/Training/I06\n","drive/My Drive/Colab Notebooks/processed_data/Training/I07\n","drive/My Drive/Colab Notebooks/processed_data/Training/I08\n","drive/My Drive/Colab Notebooks/processed_data/Training/I09\n","drive/My Drive/Colab Notebooks/processed_data/Training/I10\n","drive/My Drive/Colab Notebooks/processed_data/Training/I11\n","drive/My Drive/Colab Notebooks/processed_data/Training/I12\n","drive/My Drive/Colab Notebooks/processed_data/Training/I13\n","drive/My Drive/Colab Notebooks/processed_data/Training/I14\n","drive/My Drive/Colab Notebooks/processed_data/Training/I15\n","drive/My Drive/Colab Notebooks/processed_data/Training/I16\n","drive/My Drive/Colab Notebooks/processed_data/Training/I18\n","drive/My Drive/Colab Notebooks/processed_data/Training/I19\n","drive/My Drive/Colab Notebooks/processed_data/Training/I20\n","drive/My Drive/Colab Notebooks/processed_data/Training/I21\n","drive/My Drive/Colab Notebooks/processed_data/Training/I22\n","drive/My Drive/Colab Notebooks/processed_data/Training/I23\n","drive/My Drive/Colab Notebooks/processed_data/Training/I24\n","drive/My Drive/Colab Notebooks/processed_data/Training/I25\n","drive/My Drive/Colab Notebooks/processed_data/Training/I26\n","drive/My Drive/Colab Notebooks/processed_data/Training/I27\n","drive/My Drive/Colab Notebooks/processed_data/Training/I28\n","drive/My Drive/Colab Notebooks/processed_data/Training/I29\n","drive/My Drive/Colab Notebooks/processed_data/Training/I30\n","drive/My Drive/Colab Notebooks/processed_data/Training/I31\n","drive/My Drive/Colab Notebooks/processed_data/Training/I32\n","drive/My Drive/Colab Notebooks/processed_data/Training/I33\n","drive/My Drive/Colab Notebooks/processed_data/Training/I34\n","drive/My Drive/Colab Notebooks/processed_data/Training/I36\n","drive/My Drive/Colab Notebooks/processed_data/Training/I37\n","drive/My Drive/Colab Notebooks/processed_data/Training/I38\n","drive/My Drive/Colab Notebooks/processed_data/Training/I39\n","drive/My Drive/Colab Notebooks/processed_data/Training/I40\n","drive/My Drive/Colab Notebooks/processed_data/Training/I41\n","drive/My Drive/Colab Notebooks/processed_data/Training/I42\n","drive/My Drive/Colab Notebooks/processed_data/Training/I43\n","drive/My Drive/Colab Notebooks/processed_data/Training/I45\n","drive/My Drive/Colab Notebooks/processed_data/Training/I46\n","drive/My Drive/Colab Notebooks/processed_data/Training/I47\n","drive/My Drive/Colab Notebooks/processed_data/Training/I48\n","drive/My Drive/Colab Notebooks/processed_data/Training/I49\n","drive/My Drive/Colab Notebooks/processed_data/Training/I50\n","drive/My Drive/Colab Notebooks/processed_data/Training/I51\n","drive/My Drive/Colab Notebooks/processed_data/Training/I52\n","drive/My Drive/Colab Notebooks/processed_data/Training/I53\n","drive/My Drive/Colab Notebooks/processed_data/Training/I54\n","drive/My Drive/Colab Notebooks/processed_data/Training/I55\n","drive/My Drive/Colab Notebooks/processed_data/Training/I56\n","drive/My Drive/Colab Notebooks/processed_data/Training/I58\n","drive/My Drive/Colab Notebooks/processed_data/Training/I59\n","drive/My Drive/Colab Notebooks/processed_data/Training/I60\n","drive/My Drive/Colab Notebooks/processed_data/Training/I61\n","drive/My Drive/Colab Notebooks/processed_data/Training/I62\n","drive/My Drive/Colab Notebooks/processed_data/Training/I63\n","drive/My Drive/Colab Notebooks/processed_data/Training/I64\n","drive/My Drive/Colab Notebooks/processed_data/Training/I65\n","drive/My Drive/Colab Notebooks/processed_data/Training/I66\n","drive/My Drive/Colab Notebooks/processed_data/Training/I67\n","drive/My Drive/Colab Notebooks/processed_data/Training/I68\n","drive/My Drive/Colab Notebooks/processed_data/Training/I69\n","drive/My Drive/Colab Notebooks/processed_data/Training/I70\n","drive/My Drive/Colab Notebooks/processed_data/Training/I71\n","drive/My Drive/Colab Notebooks/processed_data/Training/I73\n","drive/My Drive/Colab Notebooks/processed_data/Training/I75\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"_r_eGfNRz1Zr","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":68},"outputId":"34b36163-2ed4-404e-eda0-6173eabe1f60","executionInfo":{"status":"ok","timestamp":1559846317350,"user_tz":-60,"elapsed":675,"user":{"displayName":"Joana Martins","photoUrl":"","userId":"01072178703692360990"}}},"source":["#Just for testing\n","print(ecgs_array.shape)\n","print(ecgs_array[0].shape)\n","np.transpose(ecgs_array[0])[:, :-1].reshape(5400, -1).reshape(5400*240, )\n","print(np.transpose(ecgs_array[0])[:, :-1].reshape(5400, -1).reshape(5400*240, ).shape)"],"execution_count":9,"outputs":[{"output_type":"stream","text":["(68, 3, 648000)\n","(3, 648000)\n","(1296000,)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"fu0bgq8Bz4P_","colab_type":"text"},"source":["## Test dataset array creation\n","\n","In this section we create the test dataset array containing all the test files. Each file has two input signals (\"MLII\",\"V1\", which correspond to signals \"II\" and \"V1\" in the training dataset) and a target signal."]},{"cell_type":"code","metadata":{"id":"NucVTffzz45Z","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":867},"outputId":"014c4d55-8f51-4953-d558-f4ce71a7b5c8","executionInfo":{"status":"ok","timestamp":1559846335467,"user_tz":-60,"elapsed":2060,"user":{"displayName":"Joana Martins","photoUrl":"","userId":"01072178703692360990"}}},"source":["test_dataset_array = []\n","\n","files_not_to_read = [110, 120, 204, 206, 211, 216, 218, 229]\n","files_not_to_read = files_not_to_read + list(range(125,200)) + list(range(224,228))\n","\n","index_counter = 0\n","for i in range(100, 235):\n","    \n","    if i not in files_not_to_read:\n","        file_path = f\"Test/{i}\"\n","        file_path = main_path + file_path\n","        print(file_path)\n","        file_data = pkl.load(open(file_path, \"rb\"))        \n","        index_counter = index_counter + 1\n","        \n","        info = [file_data[\"channelII\"], file_data[\"channelV1\"], file_data[\"label\"]]\n","        info = np.array(info)\n","        info = info.astype(np.float32)\n","        test_dataset_array.append(info)\n","\n","test_ecgs_array = np.array(test_dataset_array)\n","\n","# Test dataset length is not a multiple of 2*ninputs*seqL(rnn) which causes problems when we want to\n","# transpose the data as before, so we discard the last 2000 points \n","# The test dataset will then have the same length as the training dataset\n","# Is this ok?\n","lenRecords = test_ecgs_array.shape[2]\n","print(lenRecords)\n","new_length = int(math.floor(lenRecords/5400))*5400\n","print(new_length)\n","test_ecgs_array = test_ecgs_array[:,:,:new_length]"],"execution_count":11,"outputs":[{"output_type":"stream","text":["drive/My Drive/Colab Notebooks/processed_data/Test/100\n","drive/My Drive/Colab Notebooks/processed_data/Test/101\n","drive/My Drive/Colab Notebooks/processed_data/Test/102\n","drive/My Drive/Colab Notebooks/processed_data/Test/103\n","drive/My Drive/Colab Notebooks/processed_data/Test/104\n","drive/My Drive/Colab Notebooks/processed_data/Test/105\n","drive/My Drive/Colab Notebooks/processed_data/Test/106\n","drive/My Drive/Colab Notebooks/processed_data/Test/107\n","drive/My Drive/Colab Notebooks/processed_data/Test/108\n","drive/My Drive/Colab Notebooks/processed_data/Test/109\n","drive/My Drive/Colab Notebooks/processed_data/Test/111\n","drive/My Drive/Colab Notebooks/processed_data/Test/112\n","drive/My Drive/Colab Notebooks/processed_data/Test/113\n","drive/My Drive/Colab Notebooks/processed_data/Test/114\n","drive/My Drive/Colab Notebooks/processed_data/Test/115\n","drive/My Drive/Colab Notebooks/processed_data/Test/116\n","drive/My Drive/Colab Notebooks/processed_data/Test/117\n","drive/My Drive/Colab Notebooks/processed_data/Test/118\n","drive/My Drive/Colab Notebooks/processed_data/Test/119\n","drive/My Drive/Colab Notebooks/processed_data/Test/121\n","drive/My Drive/Colab Notebooks/processed_data/Test/122\n","drive/My Drive/Colab Notebooks/processed_data/Test/123\n","drive/My Drive/Colab Notebooks/processed_data/Test/124\n","drive/My Drive/Colab Notebooks/processed_data/Test/200\n","drive/My Drive/Colab Notebooks/processed_data/Test/201\n","drive/My Drive/Colab Notebooks/processed_data/Test/202\n","drive/My Drive/Colab Notebooks/processed_data/Test/203\n","drive/My Drive/Colab Notebooks/processed_data/Test/205\n","drive/My Drive/Colab Notebooks/processed_data/Test/207\n","drive/My Drive/Colab Notebooks/processed_data/Test/208\n","drive/My Drive/Colab Notebooks/processed_data/Test/209\n","drive/My Drive/Colab Notebooks/processed_data/Test/210\n","drive/My Drive/Colab Notebooks/processed_data/Test/212\n","drive/My Drive/Colab Notebooks/processed_data/Test/213\n","drive/My Drive/Colab Notebooks/processed_data/Test/214\n","drive/My Drive/Colab Notebooks/processed_data/Test/215\n","drive/My Drive/Colab Notebooks/processed_data/Test/217\n","drive/My Drive/Colab Notebooks/processed_data/Test/219\n","drive/My Drive/Colab Notebooks/processed_data/Test/220\n","drive/My Drive/Colab Notebooks/processed_data/Test/221\n","drive/My Drive/Colab Notebooks/processed_data/Test/222\n","drive/My Drive/Colab Notebooks/processed_data/Test/223\n","drive/My Drive/Colab Notebooks/processed_data/Test/228\n","drive/My Drive/Colab Notebooks/processed_data/Test/230\n","drive/My Drive/Colab Notebooks/processed_data/Test/231\n","drive/My Drive/Colab Notebooks/processed_data/Test/232\n","drive/My Drive/Colab Notebooks/processed_data/Test/233\n","drive/My Drive/Colab Notebooks/processed_data/Test/234\n","650000\n","648000\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"5O1mlLl9z-qX","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":85},"outputId":"f0a9bf24-f180-49f6-a8d9-debca3b44987","executionInfo":{"status":"ok","timestamp":1559846338856,"user_tz":-60,"elapsed":749,"user":{"displayName":"Joana Martins","photoUrl":"","userId":"01072178703692360990"}}},"source":["#Just for testing \n","print(test_ecgs_array.shape)\n","print(test_ecgs_array[0].shape)\n","# Reshaping would not work if the length of the records was not a multiple of 2*ninputs*seqL(rnn)\n","np.transpose(test_ecgs_array[0])[:, :-1].reshape(5400, -1).reshape(5400*240, )"],"execution_count":12,"outputs":[{"output_type":"stream","text":["(48, 3, 648000)\n","(3, 648000)\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["array([-0.01313889,  0.00847222, -0.012125  , ..., -0.03984722,\n","       -0.07179166, -0.05447222], dtype=float32)"]},"metadata":{"tags":[]},"execution_count":12}]},{"cell_type":"code","metadata":{"id":"IZgCJQX161SF","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":179},"outputId":"4695978a-bfa7-4dab-e399-e90984d941f7","executionInfo":{"status":"ok","timestamp":1559846350820,"user_tz":-60,"elapsed":5761,"user":{"displayName":"Joana Martins","photoUrl":"","userId":"01072178703692360990"}}},"source":["# number of examples\n","N = ecgs_array.shape[2]\n","\n","# Sampling frequency\n","fs = 360\n","\n","# For each timestep we give ninputs\n","ninputs = int(0.2*fs)\n","\n","# Sequence length (number of timesteps)\n","seqL = int((5 * 360)/ninputs) # Using a 5 second window sequence\n","\n","print('ninputs = ',ninputs)\n","print('seqL = ',seqL)\n","print('ninputs*seqL = ',ninputs*seqL)\n","\n","# training data for feed forward network\n","# Create efficient training sequences\n","trainData = tf.data.Dataset.from_tensors(ecgs_array[:len(ecgs_array) - 10, :, :])\n","trainData = trainData.map(lambda x:  selectFrom1ecg(x, seqL, ninputs, training = True))\n","trainData = trainData.repeat()  # Repeat the input indefinitely.\n","batchSize = 8\n","trainData = trainData.batch(batchSize)\n","\n","valData = tf.data.Dataset.from_tensors(ecgs_array[len(ecgs_array) - 10:, :, :])\n","valData = valData.map(lambda x:  selectFrom1ecg(x, seqL, ninputs, training = False))\n","valData = valData.repeat()  # Repeat the input indefinitely.\n","batchSize = 8\n","valData = valData.batch(batchSize)\n","\n","# test data for feed forward network (here we don't need to leave out the 10 files)\n","testData = tf.data.Dataset.from_tensors(test_ecgs_array)\n","testData = testData.map(lambda x:  selectFrom1ecg(x, seqL, ninputs, training = False, testing = True))\n","testData = testData.repeat()  # Repeat the input indefinitely.\n","batchSize = 8\n","testData = testData.batch(batchSize)\n","\n","\n","# Creating Training and Validation datasets with the correct shape for a Recurrent neural network\n","# The sequence length for the recurrent neural network can be about 3 times greater than for the feed\n","# forward neural net\n","seql_rnn = 3 * seqL\n","\n","print('ninputs*seqL(rnn) = ',ninputs*seql_rnn)\n","\n","trainData_rnn = tf.data.Dataset.from_tensors(ecgs_array[:len(ecgs_array) - 10, :, :])\n","trainData_rnn = trainData_rnn.map(lambda x:  selectFrom1ecg(x, seql_rnn, ninputs, training = True, feed_forward = False))\n","trainData_rnn = trainData_rnn.repeat()  # Repeat the input indefinitely.\n","batchSize_rnn = 8\n","trainData_rnn = trainData_rnn.batch(batchSize_rnn)\n","\n","valData_rnn = tf.data.Dataset.from_tensors(ecgs_array[len(ecgs_array) - 10:, :, :])\n","valData_rnn = valData_rnn.map(lambda x:  selectFrom1ecg(x, seql_rnn, ninputs, training = False, feed_forward = False))\n","valData_rnn = valData_rnn.repeat()  # Repeat the input indefinitely.\n","batchSize_rnn = 8\n","valData_rnn = valData_rnn.batch(batchSize_rnn)\n","\n","\n","# test data for feed forward network (here we don't need to leave out the 10 files)\n","testData_rnn = tf.data.Dataset.from_tensors(test_ecgs_array)\n","testData_rnn = testData_rnn.map(lambda x:  selectFrom1ecg(x, seql_rnn, ninputs, training = False, testing = True))\n","testData_rnn = testData_rnn.repeat()  # Repeat the input indefinitely.\n","batchSize = 8\n","testData_rnn = testData_rnn.batch(batchSize)"],"execution_count":13,"outputs":[{"output_type":"stream","text":["ninputs =  72\n","seqL =  25\n","ninputs*seqL =  1800\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/control_flow_ops.py:3632: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Colocations handled automatically by placer.\n","ninputs*seqL(rnn) =  5400\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"gmT6VCnJ_MhA","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"dfab5776-5c48-4369-d35f-b22a3ed53cc8","executionInfo":{"status":"ok","timestamp":1559846356022,"user_tz":-60,"elapsed":664,"user":{"displayName":"Joana Martins","photoUrl":"","userId":"01072178703692360990"}}},"source":["test_ecgs_array.shape"],"execution_count":14,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(48, 3, 648000)"]},"metadata":{"tags":[]},"execution_count":14}]},{"cell_type":"markdown","metadata":{"id":"kDLVG3AG7iAh","colab_type":"text"},"source":["## Recurrent neural network"]},{"cell_type":"code","metadata":{"id":"WOVz39jL7k93","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1020},"outputId":"15900c95-56a8-4a1a-fce2-67bde38604a6","executionInfo":{"status":"ok","timestamp":1559846612771,"user_tz":-60,"elapsed":255404,"user":{"displayName":"Joana Martins","photoUrl":"","userId":"01072178703692360990"}}},"source":["numLstmUnits = 320\n","\n","rnnModel = tf.keras.Sequential()\n","rnnModel.add(layers.CuDNNLSTM(units=numLstmUnits, return_sequences=True, input_shape = (seql_rnn, 2 * ninputs)))         \n","rnnModel.add(layers.CuDNNLSTM(units=numLstmUnits, return_sequences=True))\n","rnnModel.add(layers.CuDNNLSTM(units=numLstmUnits, return_sequences=True))         \n","rnnModel.add(layers.CuDNNLSTM(units=numLstmUnits, return_sequences=True))\n","rnnModel.add(layers.TimeDistributed(layers.Dense(ninputs)))\n","rnnModel.add(layers.Reshape((seql_rnn * ninputs, )))\n","\n","rnnModel.compile(optimizer=tf.train.RMSPropOptimizer(0.001), loss='MSE',metrics=['mae'])\n","rnnModel.fit(trainData_rnn,  epochs=10, steps_per_epoch=1000, validation_data=valData_rnn, validation_steps=100)"],"execution_count":15,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/utils/losses_utils.py:170: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use tf.cast instead.\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use tf.cast instead.\n","Epoch 1/10\n","1000/1000 [==============================] - 27s 27ms/step - loss: 0.0129 - mean_absolute_error: 0.0321 - val_loss: 0.0134 - val_mean_absolute_error: 0.0480\n","Epoch 2/10\n","1000/1000 [==============================] - 24s 24ms/step - loss: 0.0079 - mean_absolute_error: 0.0349 - val_loss: 0.0119 - val_mean_absolute_error: 0.0435\n","Epoch 3/10\n","1000/1000 [==============================] - 25s 25ms/step - loss: 0.0049 - mean_absolute_error: 0.0224 - val_loss: 0.0122 - val_mean_absolute_error: 0.0396\n","Epoch 4/10\n","1000/1000 [==============================] - 25s 25ms/step - loss: 0.0045 - mean_absolute_error: 0.0180 - val_loss: 0.0122 - val_mean_absolute_error: 0.0376\n","Epoch 5/10\n","1000/1000 [==============================] - 24s 24ms/step - loss: 0.0043 - mean_absolute_error: 0.0165 - val_loss: 0.0119 - val_mean_absolute_error: 0.0354\n","Epoch 6/10\n","1000/1000 [==============================] - 25s 25ms/step - loss: 0.0042 - mean_absolute_error: 0.0157 - val_loss: 0.0121 - val_mean_absolute_error: 0.0344\n","Epoch 7/10\n","1000/1000 [==============================] - 25s 25ms/step - loss: 0.0042 - mean_absolute_error: 0.0151 - val_loss: 0.0121 - val_mean_absolute_error: 0.0341\n","Epoch 8/10\n","1000/1000 [==============================] - 25s 25ms/step - loss: 0.0041 - mean_absolute_error: 0.0149 - val_loss: 0.0120 - val_mean_absolute_error: 0.0330\n","Epoch 9/10\n","1000/1000 [==============================] - 25s 25ms/step - loss: 0.0041 - mean_absolute_error: 0.0145 - val_loss: 0.0121 - val_mean_absolute_error: 0.0330\n","Epoch 10/10\n","1000/1000 [==============================] - 25s 25ms/step - loss: 0.0040 - mean_absolute_error: 0.0143 - val_loss: 0.0123 - val_mean_absolute_error: 0.0333\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["<tensorflow.python.keras.callbacks.History at 0x7f721a075240>"]},"metadata":{"tags":[]},"execution_count":15}]},{"cell_type":"code","metadata":{"id":"mOWmzZLOGLzl","colab_type":"code","colab":{}},"source":["# Saving the network weights\n","weights_file_path = weights_path + 'RNN_weights.h5'\n","rnnModel.save_weights(weights_file_path)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"7sJvaAvw-Khy","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":435},"outputId":"b2786b95-d3d8-474c-a0c8-e8b1f28c8189","executionInfo":{"status":"error","timestamp":1559846680001,"user_tz":-60,"elapsed":2868,"user":{"displayName":"Joana Martins","photoUrl":"","userId":"01072178703692360990"}}},"source":["out = rnnModel.evaluate(testData_rnn, steps=100)\n","print('test mean square error (loss): ', out[0], '  test absolute error: ', out[1])\n","iterator = trainData.make_initializable_iterator()\n","next_element = iterator.get_next()\n","#with tf.Session() as sess:\n","#    sess.run(iterator.initializer)\n","#    inp, targ = sess.run(next_element)\n","\n","#output = rnnModel.predict(inp)\n","\n","#t = range(seql_rnn*ninputs)\n","#plt.plot(t,inp[0,:],'k',t,targ[0]-2,'r',t,output[0]-2,'b')\n","#plt.show()"],"execution_count":18,"outputs":[{"output_type":"error","ename":"ValueError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-18-53ccdf9c5466>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrnnModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtestData_rnn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'test mean square error (loss): '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'  test absolute error: '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0miterator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainData\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_initializable_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mnext_element\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_next\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#with tf.Session() as sess:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, x, y, batch_size, verbose, sample_weight, steps, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m    987\u001b[0m         \u001b[0mcheck_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    988\u001b[0m         \u001b[0msteps_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'steps'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 989\u001b[0;31m         steps=steps)\n\u001b[0m\u001b[1;32m    990\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    991\u001b[0m     if (self.run_eagerly or (isinstance(x, iterator_ops.EagerIterator) and\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, batch_size, check_steps, steps_name, steps, validation_split, shuffle)\u001b[0m\n\u001b[1;32m   2380\u001b[0m         \u001b[0mfeed_input_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2381\u001b[0m         \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2382\u001b[0;31m         exception_prefix='input')\n\u001b[0m\u001b[1;32m   2383\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2384\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    351\u001b[0m                            \u001b[0;34m': expected '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' to have '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    352\u001b[0m                            \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' dimensions, but got array '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 353\u001b[0;31m                            'with shape ' + str(data_shape))\n\u001b[0m\u001b[1;32m    354\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    355\u001b[0m           \u001b[0mdata_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_shape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: Error when checking input: expected cu_dnnlstm_input to have 3 dimensions, but got array with shape (None, 10800)"]}]},{"cell_type":"code","metadata":{"id":"HAnEaEc2AZnq","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"9a8edf72-f613-4672-e1ad-f0ea3fd32310","executionInfo":{"status":"ok","timestamp":1559844989369,"user_tz":-60,"elapsed":662,"user":{"displayName":"Joana Martins","photoUrl":"","userId":"01072178703692360990"}}},"source":["testData_rnn"],"execution_count":18,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<DatasetV1Adapter shapes: ((?, 10800), (?, 5400)), types: (tf.float32, tf.float32)>"]},"metadata":{"tags":[]},"execution_count":18}]},{"cell_type":"markdown","metadata":{"id":"U0hloy5W7Dxe","colab_type":"text"},"source":["## Feedforward network"]},{"cell_type":"code","metadata":{"id":"3uvHOwSd7HI3","colab_type":"code","colab":{}},"source":["ffwdModel = tf.keras.Sequential()\n","ffwdModel.add(layers.Dense(64, activation='relu',input_shape=(2*seqL*ninputs,)))\n","ffwdModel.add(layers.Dense(64, activation='relu'))\n","ffwdModel.add(layers.Dense(seqL*ninputs))\n","\n","ffwdModel.compile(optimizer=tf.train.RMSPropOptimizer(0.001), loss='MSE',metrics=['mae'])\n","ffwdModel.fit(trainData,  epochs=10, steps_per_epoch=1000, validation_data=valData, validation_steps=100)"],"execution_count":0,"outputs":[]}]}