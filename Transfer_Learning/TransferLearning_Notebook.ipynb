{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer Learning\n",
    "\n",
    "This notebook implements all the steps need to achieve above 70% accuracy on a given dataset by first training a network on the mnist dataset and transfering the learned weights to the new network. This notebook also shows a comparison between applying Transfer Learning technique and training a network from scratch with few training examples as is the case with our data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the needed libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lucas/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "from keras.optimizers import SGD\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, BatchNormalization, Conv2D, MaxPooling2D\n",
    "from keras.layers import Activation, Flatten, Dropout, Dense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing the data from the mnist dataset\n",
    "\n",
    "In order to train a recurrent network on the mnist dataset we must first preprocess the data in order to be in the correct format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of epochs and batch size to user in the old model\n",
    "INIT_LR = 0.01\n",
    "NUM_EPOCHS = 5\n",
    "BS = 512\n",
    "\n",
    "# Loading the mnist data\n",
    "((train_x, train_y), (test_x, test_y)) = keras.datasets.mnist.load_data()\n",
    "\n",
    "# In this case there's only one input channel that is the black and white channel\n",
    "train_x = train_x.reshape((train_x.shape[0], 28, 28, 1))\n",
    "test_x = test_x.reshape((test_x.shape[0], 28, 28, 1))\n",
    "\n",
    "train_x = train_x.astype(\"float32\") / 255.0\n",
    "test_x = test_x.astype(\"float32\") / 255.0\n",
    "\n",
    "# one-hot encoding the trainning and testing labels\n",
    "train_y = keras.utils.to_categorical(train_y, 10)\n",
    "test_y = keras.utils.to_categorical(test_y, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the structure of the recurrent network for the mnist dataset\n",
    "\n",
    "In order to use the learned weights obtained while training the mnist dataset we must first create the structure of the network. This network structure will only be used for the mnist dataset, because with the provided dataset we will only need the learned weights and not the complete network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/lucas/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /home/lucas/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    }
   ],
   "source": [
    "# Defining the model using Keras functional API\n",
    "\n",
    "# Functional API is better for transfer learning as it supports the concatenation\n",
    "# of neural networks in a Concatenation Layer object\n",
    "# The sequential API is a more rigid API to defining neural networks\n",
    "\n",
    "# We create an input object that works like tf.placeholder\n",
    "# We add a name to the object so we can find it more easily\n",
    "inputs = Input(shape=(28, 28, 1), name=\"inputs\")\n",
    "\n",
    "# The next part of the model definition is the same as in Lab3 exercise\n",
    "# Note that these objects implement the __call__ method and can be interpreted as functions\n",
    "# The input they receive is the input the layer receives\n",
    "layer = Conv2D(32, (3, 3), padding=\"same\", input_shape=(28, 28, 1))(inputs)\n",
    "layer = Activation(\"relu\")(layer)\n",
    "layer = BatchNormalization(axis=-1)(layer)\n",
    "\n",
    "# The input layer is a convolutional layer with 32 filters\n",
    "# The shape of the kernel in this layer is 3x3\n",
    "# We add padding in this layer (so we can start the kernel right at the beginning of the image)\n",
    "# and in this case we use padding \"same\" for it to add values to the padding that are copied from the original matrix (it could also be 0)\n",
    "layer = Conv2D(32, (3, 3), padding=\"same\")(layer)\n",
    "\n",
    "# For this layer we add a ReLU activation\n",
    "# We need to add ReLU because a convolution is still a linear transformation\n",
    "# so we add ReLU for it to be a non linear transformation\n",
    "layer = Activation(\"relu\")(layer)\n",
    "\n",
    "# We add batch normalization here\n",
    "# This normalizes the output from the previous layer in order\n",
    "# for the input of the next layer to be normalized\n",
    "# In this case we put the channels at the end so we don't need to specify the axis of normalization\n",
    "# otherwise we would need to specify\n",
    "layer = BatchNormalization(axis=-1)(layer)\n",
    "\n",
    "# In this layer we Pool the layer before in order to reduce the number of features\n",
    "# Since we are using a 2x2 pooling size we are keeping only half of the features in each dimension\n",
    "# So instead of a 28*28 vector we now have a 14*14 tensor\n",
    "# Since we are omitting the stride Keras assumes the same stride as pool size which is what we want\n",
    "layer = MaxPooling2D(pool_size=(2, 2))(layer)\n",
    "\n",
    "# We add a dropout layer of 25% dropout for regularization\n",
    "layer = Dropout(0.25)(layer)\n",
    "\n",
    "# We add another convolution layer, in this case we don't need to specify the input shape\n",
    "# because keras finds out the right input shape\n",
    "layer = Conv2D(64, (3, 3), padding=\"same\")(layer)\n",
    "layer = Activation(\"relu\")(layer)\n",
    "layer = BatchNormalization(axis=-1)(layer)\n",
    "\n",
    "layer = Conv2D(64, (3, 3), padding=\"same\")(layer)\n",
    "layer = Activation(\"relu\")(layer)\n",
    "layer = BatchNormalization(axis=-1)(layer)\n",
    "\n",
    "# After this pooling we have a 7*7 tensor\n",
    "layer = MaxPooling2D(pool_size=(2, 2))(layer)\n",
    "layer = Dropout(0.25)(layer)\n",
    "\n",
    "# We add a Flatten layer in order to transform the input tensor into a vector\n",
    "# In this case we had a 7*7*64 (7*7*the number of filters we have)\n",
    "features = Flatten(name=\"features\")(layer)\n",
    "\n",
    "# Fully connected part of the network\n",
    "layer = Dense(512)(features)\n",
    "layer = Activation(\"relu\")(layer)\n",
    "layer = BatchNormalization()(layer)\n",
    "layer = Dropout(0.5)(layer)\n",
    "layer = Dense(10)(layer)\n",
    "layer = Activation(\"softmax\")(layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the network with the mnist dataset\n",
    "\n",
    "Having specified the structure of the network we can now train it on the mnist dataset.\n",
    "\n",
    "Note: After training the network we save the weights in HDF5 format in the \"files\" directory. To be more efficient and to not have to run the training of the network every time, we first verify if this directory is empty or not, if it isn't then the network has already been trained before and we don't need to train it again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we say where the model starts and ends\n",
    "old_model = Model(inputs=inputs, outputs=layer)\n",
    "\n",
    "old_model.compile(optimizer=SGD(lr=INIT_LR, momentum=0.9, decay=INIT_LR / NUM_EPOCHS), loss=\"categorical_crossentropy\",\n",
    "                  metrics=[\"accuracy\"])\n",
    "\n",
    "tensorboard_callback = keras.callbacks.TensorBoard(log_dir='./logs', histogram_freq=0,\n",
    "                                                   write_graph=True, write_images=True)\n",
    "\n",
    "if len(os.listdir(\"./files\")) == 0:\n",
    "    # If there are no files in the Files directory\n",
    "    # it means that the network hasn't been trained yet, so we\n",
    "    # need to train it and save its weights\n",
    "\n",
    "    history = old_model.fit(train_x, train_y, validation_data=(test_x, test_y), batch_size=BS, epochs=NUM_EPOCHS,\n",
    "                            callbacks=[tensorboard_callback])\n",
    "\n",
    "    # This saves the weights to the specified file in HDF5 format\n",
    "    old_model.save_weights('./files/mnist_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the learned weights to classify a new dataset\n",
    "\n",
    "Having trained the network on the mnist dataset we can now use its learned representation and apply it to a new set of data.\n",
    "\n",
    "First we need to specify the structure for the new network that will take advantage of the representations learned from the previous network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the weights previously obtained by training the network\n",
    "old_model.load_weights(\"./files/mnist_model.h5\")\n",
    "\n",
    "# We now iterate over all the layers in the model\n",
    "# In order to freeze them, we don't want to train this model\n",
    "for layer in old_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Now we create the new model, that will take advantage of the old models structure\n",
    "\n",
    "layer = Dense(512)(old_model.get_layer(\"features\").output)\n",
    "layer = Activation(\"relu\")(layer)\n",
    "layer = BatchNormalization()(layer)\n",
    "layer = Dropout(0.5)(layer)\n",
    "layer = Dense(256)(layer)\n",
    "layer = Activation(\"relu\")(layer)\n",
    "layer = BatchNormalization()(layer)\n",
    "layer = Dense(128)(layer)\n",
    "layer = Activation(\"relu\")(layer)\n",
    "layer = BatchNormalization()(layer)\n",
    "layer = Dropout(0.2)(layer)\n",
    "layer = Dense(26)(layer)\n",
    "layer = Activation(\"softmax\")(layer)\n",
    "\n",
    "# Here we say that the model starts where the old model ends\n",
    "# and ends in the layer object\n",
    "model = Model(inputs=old_model.get_layer(\"inputs\").output, outputs=layer)\n",
    "\n",
    "model.compile(optimizer=SGD(lr=1e-2, momentum=0.9), loss=\"categorical_crossentropy\",\n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After that we only need now to load the new data, do the same preprocessing we did with the mnist dataset and fit our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/lucas/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/100\n",
      "520/520 [==============================] - 2s 3ms/step - loss: 3.0654 - acc: 0.1962\n",
      "Epoch 2/100\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 1.5811 - acc: 0.5346\n",
      "Epoch 3/100\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 1.0711 - acc: 0.6692\n",
      "Epoch 4/100\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.8746 - acc: 0.7096\n",
      "Epoch 5/100\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.7456 - acc: 0.7635\n",
      "Epoch 6/100\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.5452 - acc: 0.8115\n",
      "Epoch 7/100\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.5959 - acc: 0.8000\n",
      "Epoch 8/100\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4942 - acc: 0.8385\n",
      "Epoch 9/100\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4510 - acc: 0.8442\n",
      "Epoch 10/100\n",
      "520/520 [==============================] - 1s 2ms/step - loss: 0.4557 - acc: 0.8423\n",
      "Epoch 11/100\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4376 - acc: 0.8385\n",
      "Epoch 12/100\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3222 - acc: 0.8885\n",
      "Epoch 13/100\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3603 - acc: 0.8827\n",
      "Epoch 14/100\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3260 - acc: 0.8942\n",
      "Epoch 15/100\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3494 - acc: 0.8827\n",
      "Epoch 16/100\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3103 - acc: 0.9000\n",
      "Epoch 17/100\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.2825 - acc: 0.9038\n",
      "Epoch 18/100\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.2532 - acc: 0.9096\n",
      "Epoch 19/100\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.2178 - acc: 0.9308\n",
      "Epoch 20/100\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.2432 - acc: 0.9077\n",
      "Epoch 21/100\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.2831 - acc: 0.8962\n",
      "Epoch 22/100\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.2327 - acc: 0.9135\n",
      "Epoch 23/100\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.2363 - acc: 0.9327\n",
      "Epoch 24/100\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.1880 - acc: 0.9442\n",
      "Epoch 25/100\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.1563 - acc: 0.9462\n",
      "Epoch 26/100\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.1981 - acc: 0.9308\n",
      "Epoch 27/100\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.2127 - acc: 0.9308\n",
      "Epoch 28/100\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.1419 - acc: 0.9577\n",
      "Epoch 29/100\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.1862 - acc: 0.9365\n",
      "Epoch 30/100\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.1646 - acc: 0.9346\n",
      "Epoch 31/100\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.2130 - acc: 0.9288\n",
      "Epoch 32/100\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.1444 - acc: 0.9462\n",
      "Epoch 33/100\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.1414 - acc: 0.9519\n",
      "Epoch 34/100\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.2020 - acc: 0.9346\n",
      "Epoch 35/100\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.1648 - acc: 0.9442\n",
      "Epoch 36/100\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.1730 - acc: 0.9346\n",
      "Epoch 37/100\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.1908 - acc: 0.9250\n",
      "Epoch 38/100\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.1739 - acc: 0.9423\n",
      "Epoch 39/100\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.1693 - acc: 0.9365\n",
      "Epoch 40/100\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.1558 - acc: 0.9462\n",
      "Epoch 41/100\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.1506 - acc: 0.9423\n",
      "Epoch 42/100\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.1098 - acc: 0.9673\n",
      "Epoch 43/100\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.1551 - acc: 0.9462\n",
      "Epoch 44/100\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.1496 - acc: 0.9558\n",
      "Epoch 45/100\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.1846 - acc: 0.9346\n",
      "Epoch 46/100\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.1389 - acc: 0.9481\n",
      "Epoch 47/100\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.1276 - acc: 0.9538\n",
      "Epoch 48/100\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.1379 - acc: 0.9596\n",
      "Epoch 49/100\n",
      "520/520 [==============================] - 1s 2ms/step - loss: 0.0874 - acc: 0.9731\n",
      "Epoch 50/100\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.1222 - acc: 0.9673\n",
      "Epoch 51/100\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.0972 - acc: 0.9692\n",
      "Epoch 52/100\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.0972 - acc: 0.9673\n",
      "Epoch 53/100\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.1252 - acc: 0.9596\n",
      "Epoch 54/100\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.1364 - acc: 0.9538\n",
      "Epoch 55/100\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.1052 - acc: 0.9673\n",
      "Epoch 56/100\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.1089 - acc: 0.9673\n",
      "Epoch 57/100\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.0723 - acc: 0.9750\n",
      "Epoch 58/100\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.0739 - acc: 0.9808\n",
      "Epoch 59/100\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.1142 - acc: 0.9615\n",
      "Epoch 60/100\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.0949 - acc: 0.9692\n",
      "Epoch 61/100\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.0869 - acc: 0.9750\n",
      "Epoch 62/100\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.0686 - acc: 0.9788\n",
      "Epoch 63/100\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.0847 - acc: 0.9673\n",
      "Epoch 64/100\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.0925 - acc: 0.9654\n",
      "Epoch 65/100\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.0828 - acc: 0.9769\n",
      "Epoch 66/100\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.0962 - acc: 0.9596\n",
      "Epoch 67/100\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.0820 - acc: 0.9769\n",
      "Epoch 68/100\n",
      "520/520 [==============================] - 1s 2ms/step - loss: 0.0957 - acc: 0.9673\n",
      "Epoch 69/100\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.0842 - acc: 0.9673\n",
      "Epoch 70/100\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.0952 - acc: 0.9712\n",
      "Epoch 71/100\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.0856 - acc: 0.9731\n",
      "Epoch 72/100\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.0627 - acc: 0.9846\n",
      "Epoch 73/100\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.0682 - acc: 0.9788\n",
      "Epoch 74/100\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.0707 - acc: 0.9731\n",
      "Epoch 75/100\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.0618 - acc: 0.9808\n",
      "Epoch 76/100\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.0634 - acc: 0.9808\n",
      "Epoch 77/100\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.0619 - acc: 0.9827\n",
      "Epoch 78/100\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.1048 - acc: 0.9712\n",
      "Epoch 79/100\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.0639 - acc: 0.9750\n",
      "Epoch 80/100\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.0718 - acc: 0.9692\n",
      "Epoch 81/100\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.0701 - acc: 0.9712\n",
      "Epoch 82/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "520/520 [==============================] - 1s 1ms/step - loss: 0.0725 - acc: 0.9712\n",
      "Epoch 83/100\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.0848 - acc: 0.9635\n",
      "Epoch 84/100\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.0737 - acc: 0.9692\n",
      "Epoch 85/100\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.0699 - acc: 0.9750\n",
      "Epoch 86/100\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.0585 - acc: 0.9846\n",
      "Epoch 87/100\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.0468 - acc: 0.9865\n",
      "Epoch 88/100\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.0686 - acc: 0.9865\n",
      "Epoch 89/100\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.0457 - acc: 0.9865\n",
      "Epoch 90/100\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.0591 - acc: 0.9788\n",
      "Epoch 91/100\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.0446 - acc: 0.9827\n",
      "Epoch 92/100\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.0524 - acc: 0.9865\n",
      "Epoch 93/100\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.0668 - acc: 0.9788\n",
      "Epoch 94/100\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.0611 - acc: 0.9788\n",
      "Epoch 95/100\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.0722 - acc: 0.9692\n",
      "Epoch 96/100\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.0574 - acc: 0.9769\n",
      "Epoch 97/100\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.0871 - acc: 0.9692\n",
      "Epoch 98/100\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.0589 - acc: 0.9827\n",
      "Epoch 99/100\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.0641 - acc: 0.9808\n",
      "Epoch 100/100\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.0506 - acc: 0.9846\n"
     ]
    }
   ],
   "source": [
    "# Loading the new data\n",
    "new_train_x = np.load('./data/imagesLettersTrain.npy')\n",
    "new_train_y = np.load('./data/labelsTrain.npy')\n",
    "\n",
    "new_test_x = np.load('./data/imagesLettersTest.npy')\n",
    "new_test_y = np.load('./data/labelsTest.npy')\n",
    "\n",
    "# In this case we only have one input channel that is the black and white channel\n",
    "new_train_x = new_train_x.reshape((new_train_x.shape[0], 28, 28, 1))\n",
    "new_test_x = new_test_x.reshape((new_test_x.shape[0], 28, 28, 1))\n",
    "\n",
    "new_train_x = new_train_x.astype(\"float32\") / 255.0\n",
    "new_test_x = new_test_x.astype(\"float32\") / 255.0\n",
    "\n",
    "# We one-hot encode the trainning and testing labels\n",
    "# Now we have 26 different labels so we one-hot encode a vector with size 26\n",
    "new_train_y = keras.utils.to_categorical(new_train_y, 26)\n",
    "new_test_y = keras.utils.to_categorical(new_test_y, 26)\n",
    "\n",
    "NEW_NUM_EPOCHS = 100\n",
    "NEW_BS = 20\n",
    "\n",
    "fitting = model.fit(new_train_x, new_train_y, batch_size=NEW_BS, epochs=NEW_NUM_EPOCHS, callbacks=[tensorboard_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having fined tuned our model to the new training set we can see that it achieves an accuracy of around 97%. This is a really great value considering the few number of examples existent in the training set. We must now evaluate how well our model does on the test set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "103080/103080 [==============================] - 94s 910us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.9755157116280133, 0.7765327900659682]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(x = new_test_x, y = new_test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results above are:\n",
    "\n",
    "| Test Loss | Test Accuracy |\n",
    "|-----------|---------------|\n",
    "| 0.976     | 0.777         |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a network from scratch with the new dataset\n",
    "\n",
    "In order to better example the benefits of Transfer Learning we will compare the previous results obtained with transfer learning with the results of training a network from scratch. \n",
    "\n",
    "Training a network from scratch is usually the prefered method of training a network, as in this way the model can learn all the detail specific for a given dataset, but when there's very few training examples, as is the case for this dataset, it may not be enough to accuratly train a neural networok and so transfer learning may be an option\n",
    "\n",
    "We will build a similar network structure as the one built for training on the mnist dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the model using Keras functional API\n",
    "\n",
    "# Functional API is better for transfer learning as it supports the concatenation\n",
    "# of neural networks in a Concatenation Layer object\n",
    "# The sequential API is a more rigid API to defining neural networks\n",
    "\n",
    "# We create an input object that works like tf.placeholder\n",
    "# We add a name to the object so we can find it more easily\n",
    "new_inputs = Input(shape=(28, 28, 1), name=\"inputs\")\n",
    "\n",
    "# The next part of the model definition is the same as in Lab3 exercise\n",
    "# Note that these objects implement the __call__ method and can be interpreted as functions\n",
    "# The input they receive is the input the layer receives\n",
    "new_layer = Conv2D(32, (3, 3), padding=\"same\", input_shape=(28, 28, 1))(new_inputs)\n",
    "new_layer = Activation(\"relu\")(new_layer)\n",
    "new_layer = BatchNormalization(axis=-1)(new_layer)\n",
    "\n",
    "# The input layer is a convolutional layer with 32 filters\n",
    "# The shape of the kernel in this layer is 3x3\n",
    "# We add padding in this layer (so we can start the kernel right at the beginning of the image)\n",
    "# and in this case we use padding \"same\" for it to add values to the padding that are copied from the original matrix (it could also be 0)\n",
    "new_layer = Conv2D(32, (3, 3), padding=\"same\")(new_layer)\n",
    "\n",
    "# For this layer we add a ReLU activation\n",
    "# We need to add ReLU because a convolution is still a linear transformation\n",
    "# so we add ReLU for it to be a non linear transformation\n",
    "new_layer = Activation(\"relu\")(new_layer)\n",
    "\n",
    "# We add batch normalization here\n",
    "# This normalizes the output from the previous layer in order\n",
    "# for the input of the next layer to be normalized\n",
    "# In this case we put the channels at the end so we don't need to specify the axis of normalization\n",
    "# otherwise we would need to specify\n",
    "new_layer = BatchNormalization(axis=-1)(new_layer)\n",
    "\n",
    "# In this layer we Pool the layer before in order to reduce the number of features\n",
    "# Since we are using a 2x2 pooling size we are keeping only half of the features in each dimension\n",
    "# So instead of a 28*28 vector we now have a 14*14 tensor\n",
    "# Since we are omitting the stride Keras assumes the same stride as pool size which is what we want\n",
    "new_layer = MaxPooling2D(pool_size=(2, 2))(new_layer)\n",
    "\n",
    "# We add a dropout layer of 25% dropout for regularization\n",
    "new_layer = Dropout(0.25)(new_layer)\n",
    "\n",
    "# We add another convolution layer, in this case we don't need to specify the input shape\n",
    "# because keras finds out the right input shape\n",
    "new_layer = Conv2D(64, (3, 3), padding=\"same\")(new_layer)\n",
    "new_layer = Activation(\"relu\")(new_layer)\n",
    "new_layer = BatchNormalization(axis=-1)(new_layer)\n",
    "\n",
    "new_layer = Conv2D(64, (3, 3), padding=\"same\")(new_layer)\n",
    "new_layer = Activation(\"relu\")(new_layer)\n",
    "new_layer = BatchNormalization(axis=-1)(new_layer)\n",
    "\n",
    "# After this pooling we have a 7*7 tensor\n",
    "new_layer = MaxPooling2D(pool_size=(2, 2))(new_layer)\n",
    "new_layer = Dropout(0.25)(new_layer)\n",
    "\n",
    "# We add a Flatten layer in order to transform the input tensor into a vector\n",
    "# In this case we had a 7*7*64 (7*7*the number of filters we have)\n",
    "features = Flatten(name=\"features\")(new_layer)\n",
    "\n",
    "# Fully connected part of the network\n",
    "new_layer = Dense(512)(features)\n",
    "new_layer = Activation(\"relu\")(new_layer)\n",
    "new_layer = BatchNormalization()(new_layer)\n",
    "new_layer = Dropout(0.5)(new_layer)\n",
    "new_layer = Dense(10)(new_layer)\n",
    "new_layer = Activation(\"softmax\")(new_layer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
